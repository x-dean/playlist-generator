# Processing Logic - File Size Based Strategy

## ðŸŽ¯ **New Processing Logic**

### **Rule: If file is over 200MB and not in failed then use sequential, then parallel**

## ðŸ“Š **How It Works**

### **1. File Classification**
```python
# Files are classified by size from database
BIG_FILE_SIZE_MB = 200  # 200MB threshold

# Get file sizes from database (stored during file discovery)
file_sizes = audio_db.get_file_sizes_from_db(file_paths_only)

for file_path in files_to_analyze:
    file_size_bytes = file_sizes.get(file_path, 0)
    file_size_mb = file_size_bytes / (1024 * 1024)
    
    if file_size_mb > BIG_FILE_SIZE_MB:
        big_files.append(file_path)      # > 200MB
    else:
        normal_files.append(file_path)   # â‰¤ 200MB
```

### **2. Processing Strategy**

#### **Step 1: Sequential Processing (Big Files)**
```python
if big_files:
    logger.info(f"Processing {len(big_files)} big files sequentially...")
    sequential_processor = SequentialProcessor()
    workers = 1  # Sequential processing uses 1 worker
    
    for features, filepath, db_write_success in sequential_processor.process(big_files, workers, force_reextract=force_reextract):
        all_results.append((features, filepath, db_write_success))
```

#### **Step 2: Parallel Processing (Normal Files)**
```python
if normal_files:
    logger.info(f"Processing {len(normal_files)} normal files in parallel...")
    parallel_processor = ParallelProcessor()
    workers = args.workers or max(1, mp.cpu_count())
    
    for features, filepath, db_write_success in parallel_processor.process(normal_files, workers, force_reextract=force_reextract):
        all_results.append((features, filepath, db_write_success))
```

## ðŸ”„ **Processing Flow**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   File List     â”‚    â”‚   Size Check    â”‚    â”‚   Processing    â”‚
â”‚   (To Analyze)  â”‚â”€â”€â”€â–¶â”‚   (200MB)       â”‚â”€â”€â”€â–¶â”‚   Strategy      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                       â”‚
                                â–¼                       â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Big Files     â”‚    â”‚   Normal Files  â”‚
                    â”‚   (> 200MB)     â”‚    â”‚   (â‰¤ 200MB)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                       â”‚
                                â–¼                       â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Sequential     â”‚    â”‚   Parallel      â”‚
                    â”‚  Processing     â”‚    â”‚   Processing    â”‚
                    â”‚  (1 worker)     â”‚    â”‚   (N workers)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                       â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         Combined Results            â”‚
                    â”‚     (Sequential + Parallel)        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“ˆ **Benefits**

### **âœ… Optimal Resource Usage**
- **Big files** â†’ Sequential (avoids memory issues)
- **Normal files** â†’ Parallel (maximizes CPU usage)

### **âœ… Memory Management**
- Large files processed one at a time
- Prevents memory overflow with big files
- Efficient for mixed file sizes

### **âœ… Performance Optimization**
- Small files benefit from parallel processing
- Big files don't block the queue
- Better overall throughput

## ðŸŽ¯ **Example Scenarios**

### **Scenario 1: Mixed File Sizes**
```
Files to process:
- song1.mp3 (5MB)     â†’ Normal file â†’ Parallel
- song2.mp3 (8MB)     â†’ Normal file â†’ Parallel  
- album.flac (250MB)  â†’ Big file   â†’ Sequential
- song3.mp3 (3MB)     â†’ Normal file â†’ Parallel

Processing order:
1. album.flac (Sequential, 1 worker)
2. song1.mp3, song2.mp3, song3.mp3 (Parallel, N workers)
```

### **Scenario 2: All Small Files**
```
Files to process:
- song1.mp3 (5MB)  â†’ Normal file â†’ Parallel
- song2.mp3 (8MB)  â†’ Normal file â†’ Parallel
- song3.mp3 (3MB)  â†’ Normal file â†’ Parallel

Processing: All files processed in parallel
```

### **Scenario 3: All Big Files**
```
Files to process:
- album1.flac (300MB) â†’ Big file â†’ Sequential
- album2.flac (250MB) â†’ Big file â†’ Sequential

Processing: All files processed sequentially (one at a time)
```

## ðŸ”§ **Configuration**

### **Threshold Setting**
```python
BIG_FILE_SIZE_MB = 200  # 200MB threshold
```

### **Worker Configuration**
```python
# Sequential processing (big files)
workers = 1

# Parallel processing (normal files)  
workers = args.workers or max(1, mp.cpu_count())
```

## ðŸ—„ï¸ **Database Integration**

### **File Size Storage**
File sizes are stored in the `file_discovery_state` table during file discovery:

```sql
CREATE TABLE file_discovery_state (
    file_path TEXT PRIMARY KEY,
    file_hash TEXT,
    file_size INTEGER,        -- File size in bytes
    last_modified REAL,
    discovered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_seen_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status TEXT DEFAULT 'active'
)
```

### **Size Retrieval**
```python
# Get file sizes from database instead of filesystem
file_sizes = audio_db.get_file_sizes_from_db(file_paths_only)

# Use stored sizes for classification
for file_path in file_paths_only:
    file_size_bytes = file_sizes.get(file_path, 0)
    file_size_mb = file_size_bytes / (1024 * 1024)
    # ... classification logic
```

### **Benefits**
- âœ… **Performance**: No filesystem I/O during analysis
- âœ… **Consistency**: Same size data used across runs
- âœ… **Efficiency**: Database queries are faster than stat() calls
- âœ… **Reliability**: No filesystem access errors during processing

## ðŸ“Š **Logging Output**

```
File distribution: 15 normal files, 2 big files (>200MB)
Processing 2 big files sequentially...
Processing 15 normal files in parallel...
```

## âœ… **Verification**

The new logic ensures:
- âœ… Files > 200MB are processed sequentially
- âœ… Files â‰¤ 200MB are processed in parallel
- âœ… Failed files are excluded from processing
- âœ… Optimal resource usage for different file sizes
- âœ… Clear logging of processing strategy

This approach provides the best balance of performance and resource management! ðŸš€ 